{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline for C100 Fault Classification \n",
    "_February 24, 2020_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to get an idea of run time\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to put together a rough impementation of a data pipeline for CEBAF C100 cavity and fault detection. There are two questions that need to be addressed:\n",
    "<br>\n",
    "\n",
    "  - *which cavity tripped first?* <br>\n",
    "  - *which type of fault caused the trip?*\n",
    "\n",
    "This implementation uses machine learning (as opposed to deep learning) and suffers from the fact that feature extraction and selection is computationally expensive. Nevertheless, it provides the opportunity to implement a functional pipeline on a timescale that can be used for the winter 2020 running period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Cavity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the onset of a cavity trip, the waveform harvester will collect and write data to file. The data collected will be 17 RF signals for each of the 8 cavities in the offending C100 cryomodule. To make the feature engineering manageable (i.e. less computationally expensive) we do the following:\n",
    "\n",
    "- keep only 4 of the 17 signals for each of the 8 cavities\n",
    "    - based on analysis from subject matter experts, the most relevant signals are (GASK, GMES, CRFP, DETA2)\n",
    "- for each signal use `statsmodels` to compute autoregressive features\n",
    "    - still have not figured out why features computed by `tsfresh` are not effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present I am mimicking the file structure for the data as it appears in `M:\\asd\\asddata\\FCCWaveforms\\Spring 2018\\rf`. Therefore the script to read in the data will need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `dictionary` to use consistent nomenclature for the cryomodule name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1611,
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_dict = {'0L04': 'R04', '1L22': 'R1M', '1L23': 'R1N', '1L24': 'R1O', '1L25': 'R1P',\n",
    "            '1L26': 'R1Q', '2L22': 'R2M', '2L23': 'R2N', '2L24': 'R2O', '2L25': 'R2P', '2L26': 'R2Q'}\n",
    "\n",
    "cavity_df = pd.DataFrame()\n",
    "module_path = Path('D:/RF WAVEFORMS/rfw_tsf_extractor-Spring-2019/waveform-data/rf')\n",
    "dir = Path('D:/RF WAVEFORMS/rfw_tsf_extractor-Spring-2019/labeled-examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For developing machine learning models, a list of `fault_*.txt` files were read in. Each file was generated by a subject matter expert and contained a list of trip events. For each event, the cavity and fault label were given as well as the timestamp associated with the fault. As a test of the system, we read in a similar file containing only one event and ignore the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1612,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = ['example.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is ugly but the idea is that after reading in and parsing the timestamp from the file above, it searches the directories and collects the appropriate cavity signals and stores the labels (the cavity number which tripped). With the real-time system we will not have labels (all actions associated with `y` have been commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1613,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 19), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 38), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 57), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 76), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 95), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 114), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 133), time step = 0.05 ms\n",
      "j = 0, k = 1, input shape = (8192, 19), cavity shape = (8192, 152), time step = 0.05 ms\n"
     ]
    }
   ],
   "source": [
    "#y = pd.Series([])\n",
    "\n",
    "k = 0\n",
    "for i in filelist:\n",
    "    data_file_path = dir/i # IMPORTANT to add sub-directores to `module_path`\n",
    "\n",
    "    log = pd.read_table(data_file_path, sep='\\t')\n",
    "\n",
    "    m, n = log.shape\n",
    "    \n",
    "    for j in range(0, m):\n",
    "        k += 1\n",
    "        date, time = log.time[j].split(\" \", 1)\n",
    "        \n",
    "        # formatting the timestamp in the .txt file to match the format used in the filenames\n",
    "        date_format = date.replace(\"/\", \"_\")\n",
    "        time_format = time.replace(\":\", \"\")\n",
    "\n",
    "        list1 = [time_format, '.', '?']\n",
    "\n",
    "        ct = os.path.join(module_path, log.zone[j], date_format, \"\".join(list1))\n",
    "        \n",
    "        # output of glob.glob() is a list; rather than convert to string and remove brackets and quotes, select first element\n",
    "        dir1 = glob.glob(ct)\n",
    "        dir2 = os.listdir(dir1[0]) # list of filenames in the directory\n",
    "    \n",
    "        module_df = pd.DataFrame()\n",
    "        \n",
    "        # only read in data if all 8 cavity files are present in directory\n",
    "        if len(dir2) == 8:\n",
    "\n",
    "            for m in range(0,8):\n",
    "                f = os.path.join(dir1[0], dir2[m])\n",
    "                df = pd.read_table(f, sep='\\t')\n",
    "                sLength = len(df['Time'])\n",
    "                tStep = (df.Time[2] - df.Time[1]) # in milliseconds\n",
    "                df['id'] = pd.Series(k, index=df.index)\n",
    "                col = ['Time', \n",
    "                      f'{m+1}_IMES', f'{m+1}_QMES', f'{m+1}_GMES', f'{m+1}_PMES', f'{m+1}_IASK', f'{m+1}_QASK', \n",
    "                      f'{m+1}_GASK', f'{m+1}_PASK', f'{m+1}_CRFP', f'{m+1}_CRFPP', f'{m+1}_CRRP', f'{m+1}_CRRPP', \n",
    "                      f'{m+1}_GLDE', f'{m+1}_PLDE', f'{m+1}_DETA2_', f'{m+1}_CFQE2_', f'{m+1}_DFQES',\n",
    "                      'id']\n",
    "                df.columns=col\n",
    "                module_df = pd.concat([module_df, df], axis=1, sort=False)\n",
    "                #print(\"Path: {0:s}\".format(f))\n",
    "                print(\"j = {0:}, k = {1:}, input shape = {2:}, cavity shape = {3:}, time step = {4:3.2f} ms\".format(j, k, df.shape, module_df.shape, tStep))\n",
    "            module_df = module_df.loc[:,~module_df.columns.duplicated()]\n",
    "            #y_tmp = pd.Series(log.cavity[j], index=[k])\n",
    "            #y = y.append(y_tmp)\n",
    "            \n",
    "        else:\n",
    "            print(\"Directory did not contain data files for all 8 cavities in the zone.\")\n",
    "\n",
    "        cavity_df = cavity_df.append(module_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce computational burden, we select only the (GMES, GASK, CRFP, DETA2) signals from each cavity (i.e. `<cavID>_<signal>`). (Note, the `CRFPP` signal was inadvertantly used in the training of the model, so we leave it in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = [\"1_GMES\", \"1_GASK\", \"1_CRFP\", \"1_CRFPP\", \"1_DETA2_\", \"2_GMES\", \"2_GASK\", \"2_CRFP\", \"2_CRFPP\", \"2_DETA2_\",\n",
    "           \"3_GMES\", \"3_GASK\", \"3_CRFP\", \"3_CRFPP\", \"3_DETA2_\", \"4_GMES\", \"4_GASK\", \"4_CRFP\", \"4_CRFPP\", \"4_DETA2_\",\n",
    "           \"5_GMES\", \"5_GASK\", \"5_CRFP\", \"5_CRFPP\", \"5_DETA2_\", \"6_GMES\", \"6_GASK\", \"6_CRFP\", \"6_CRFPP\", \"6_DETA2_\",\n",
    "           \"7_GMES\", \"7_GASK\", \"7_CRFP\", \"7_CRFPP\", \"7_DETA2_\", \"8_GMES\", \"8_GASK\", \"8_CRFP\", \"8_CRFPP\", \"8_DETA2_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [],
   "source": [
    "cavity_df = cavity_df[sel_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_GMES</th>\n",
       "      <th>1_GASK</th>\n",
       "      <th>1_CRFP</th>\n",
       "      <th>1_CRFPP</th>\n",
       "      <th>1_DETA2_</th>\n",
       "      <th>2_GMES</th>\n",
       "      <th>2_GASK</th>\n",
       "      <th>2_CRFP</th>\n",
       "      <th>2_CRFPP</th>\n",
       "      <th>2_DETA2_</th>\n",
       "      <th>...</th>\n",
       "      <th>7_GMES</th>\n",
       "      <th>7_GASK</th>\n",
       "      <th>7_CRFP</th>\n",
       "      <th>7_CRFPP</th>\n",
       "      <th>7_DETA2_</th>\n",
       "      <th>8_GMES</th>\n",
       "      <th>8_GASK</th>\n",
       "      <th>8_CRFP</th>\n",
       "      <th>8_CRFPP</th>\n",
       "      <th>8_DETA2_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0053</td>\n",
       "      <td>2.90009</td>\n",
       "      <td>2.99309</td>\n",
       "      <td>-116.884</td>\n",
       "      <td>-1.65894</td>\n",
       "      <td>18.5081</td>\n",
       "      <td>4.60938</td>\n",
       "      <td>3.84167</td>\n",
       "      <td>-104.980</td>\n",
       "      <td>-5.20752</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0102</td>\n",
       "      <td>3.64624</td>\n",
       "      <td>1.50030</td>\n",
       "      <td>143.212</td>\n",
       "      <td>-2.73560</td>\n",
       "      <td>12.5082</td>\n",
       "      <td>2.99469</td>\n",
       "      <td>1.97090</td>\n",
       "      <td>-101.3320</td>\n",
       "      <td>-6.91040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0081</td>\n",
       "      <td>2.76337</td>\n",
       "      <td>2.92754</td>\n",
       "      <td>-116.856</td>\n",
       "      <td>-1.68640</td>\n",
       "      <td>18.5081</td>\n",
       "      <td>4.67651</td>\n",
       "      <td>3.77528</td>\n",
       "      <td>-105.606</td>\n",
       "      <td>-5.28992</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0027</td>\n",
       "      <td>3.83789</td>\n",
       "      <td>1.76222</td>\n",
       "      <td>148.722</td>\n",
       "      <td>-2.40051</td>\n",
       "      <td>12.5028</td>\n",
       "      <td>3.15735</td>\n",
       "      <td>1.99497</td>\n",
       "      <td>-99.0088</td>\n",
       "      <td>-6.66321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0059</td>\n",
       "      <td>3.01025</td>\n",
       "      <td>2.86854</td>\n",
       "      <td>-114.274</td>\n",
       "      <td>-1.07666</td>\n",
       "      <td>18.5028</td>\n",
       "      <td>4.82574</td>\n",
       "      <td>3.71025</td>\n",
       "      <td>-103.063</td>\n",
       "      <td>-4.65820</td>\n",
       "      <td>...</td>\n",
       "      <td>16.9990</td>\n",
       "      <td>4.04114</td>\n",
       "      <td>1.51870</td>\n",
       "      <td>145.107</td>\n",
       "      <td>-3.12561</td>\n",
       "      <td>12.5082</td>\n",
       "      <td>3.10699</td>\n",
       "      <td>2.03230</td>\n",
       "      <td>-103.2550</td>\n",
       "      <td>-6.49292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0081</td>\n",
       "      <td>2.74963</td>\n",
       "      <td>2.95594</td>\n",
       "      <td>-117.202</td>\n",
       "      <td>-1.57104</td>\n",
       "      <td>18.5024</td>\n",
       "      <td>4.70093</td>\n",
       "      <td>3.83083</td>\n",
       "      <td>-103.629</td>\n",
       "      <td>-5.16907</td>\n",
       "      <td>...</td>\n",
       "      <td>16.9868</td>\n",
       "      <td>4.42383</td>\n",
       "      <td>1.71018</td>\n",
       "      <td>145.223</td>\n",
       "      <td>-3.59802</td>\n",
       "      <td>12.5038</td>\n",
       "      <td>3.13293</td>\n",
       "      <td>1.98448</td>\n",
       "      <td>-103.1840</td>\n",
       "      <td>-6.88843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0064</td>\n",
       "      <td>2.80609</td>\n",
       "      <td>2.96942</td>\n",
       "      <td>-113.967</td>\n",
       "      <td>-1.74133</td>\n",
       "      <td>18.5081</td>\n",
       "      <td>4.71588</td>\n",
       "      <td>3.81863</td>\n",
       "      <td>-105.980</td>\n",
       "      <td>-5.10315</td>\n",
       "      <td>...</td>\n",
       "      <td>16.9971</td>\n",
       "      <td>4.08417</td>\n",
       "      <td>1.56034</td>\n",
       "      <td>144.899</td>\n",
       "      <td>-3.58154</td>\n",
       "      <td>12.5097</td>\n",
       "      <td>3.07190</td>\n",
       "      <td>2.08105</td>\n",
       "      <td>-101.4150</td>\n",
       "      <td>-6.73462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1_GMES   1_GASK   1_CRFP  1_CRFPP  1_DETA2_   2_GMES   2_GASK   2_CRFP  \\\n",
       "0  18.0053  2.90009  2.99309 -116.884  -1.65894  18.5081  4.60938  3.84167   \n",
       "1  18.0081  2.76337  2.92754 -116.856  -1.68640  18.5081  4.67651  3.77528   \n",
       "2  18.0059  3.01025  2.86854 -114.274  -1.07666  18.5028  4.82574  3.71025   \n",
       "3  18.0081  2.74963  2.95594 -117.202  -1.57104  18.5024  4.70093  3.83083   \n",
       "4  18.0064  2.80609  2.96942 -113.967  -1.74133  18.5081  4.71588  3.81863   \n",
       "\n",
       "   2_CRFPP  2_DETA2_    ...      7_GMES   7_GASK   7_CRFP  7_CRFPP  7_DETA2_  \\\n",
       "0 -104.980  -5.20752    ...     17.0102  3.64624  1.50030  143.212  -2.73560   \n",
       "1 -105.606  -5.28992    ...     17.0027  3.83789  1.76222  148.722  -2.40051   \n",
       "2 -103.063  -4.65820    ...     16.9990  4.04114  1.51870  145.107  -3.12561   \n",
       "3 -103.629  -5.16907    ...     16.9868  4.42383  1.71018  145.223  -3.59802   \n",
       "4 -105.980  -5.10315    ...     16.9971  4.08417  1.56034  144.899  -3.58154   \n",
       "\n",
       "    8_GMES   8_GASK   8_CRFP   8_CRFPP  8_DETA2_  \n",
       "0  12.5082  2.99469  1.97090 -101.3320  -6.91040  \n",
       "1  12.5028  3.15735  1.99497  -99.0088  -6.66321  \n",
       "2  12.5082  3.10699  2.03230 -103.2550  -6.49292  \n",
       "3  12.5038  3.13293  1.98448 -103.1840  -6.88843  \n",
       "4  12.5097  3.07190  2.08105 -101.4150  -6.73462  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 1616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cavity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cavity_df = cavity_df.rename(columns={\"1_DETA2_\":\"1_DETA2\", \"2_DETA2_\":\"2_DETA2\", \"3_DETA2_\":\"3_DETA2\", \"4_DETA2_\":\"4_DETA2\",\n",
    "#                          \"5_DETA2_\":\"5_DETA2\", \"6_DETA2_\":\"6_DETA2\", \"7_DETA2_\":\"7_DETA2\", \"8_DETA2_\":\"8_DETA2\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few diagnostic checks on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input file has 8192 rows and 40 columns\n",
      "The input file has 1.0 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"The input file has\", cavity_df.shape[0], \"rows and\", cavity_df.shape[1], \"columns\")\n",
    "print(\"The input file has\", cavity_df.shape[0]/8192, \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatARCoreffs(signals, maxLag):\n",
    "    for i in range(0, np.shape(signals)[1]):\n",
    "        model = AR(signals[:, i])\n",
    "        model_fit = model.fit(maxlag=5, ic=None)\n",
    "        if np.shape(model_fit.params)[0] < maxLag + 1:\n",
    "            parameters = np.pad(model_fit.params, (0, maxLag + 1 - np.shape(model_fit.params)[0]), 'constant', constant_values=0)\n",
    "        elif np.shape(model_fit.params)[0] > maxLag + 1:\n",
    "            parameters = model_fit.params[: maxLag]\n",
    "        else:\n",
    "            parameters = model_fit.params\n",
    "\n",
    "        if i == 0:\n",
    "            coefficients = parameters\n",
    "        else:\n",
    "            coefficients = np.append(coefficients, parameters, axis=0)\n",
    "\n",
    "    return pd.DataFrame(coefficients).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "signalScaler = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "cavity_df[sel_col] = signalScaler.fit_transform(cavity_df[sel_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cavity_master = getStatARCoreffs(cavity_df.values, maxLag=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 240)"
      ]
     },
     "execution_count": 1623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cavity_master.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few diagnostic checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1\n",
      "Number of features: 240\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: {}\".format(X_cavity_master.shape[0]))\n",
    "print(\"Number of features: {}\".format(X_cavity_master.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `joblib` to import previously saved model for determining which cavity tripped. The model is based on a `RandomForestClassifier` using the following parameters found using `GridSearchCV`:\n",
    "- `n_estimators` = X\n",
    "- `max_depth` = X\n",
    "- `min_samples_split` = X\n",
    "- `max_features` = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "RF_cavity = joblib.load('RF_CAVITY_AR_02202020.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cavity 2 was the first to go unstable\n",
      "Confidence level is: 85.21794871794872 %\n"
     ]
    }
   ],
   "source": [
    "cavityID = RF_cavity.predict(X_cavity_master)\n",
    "cavityID_prob = RF_cavity.predict_proba(X_cavity_master)\n",
    "cavityID_str = cavityID.astype(str)[0]\n",
    "print(\"Cavity\", cavityID_str, \"was the first to go unstable\")\n",
    "ID_confidence = float(cavityID_prob[0][cavityID]*100)\n",
    "print(\"Confidence level is:\", ID_confidence,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Fault?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, to make the feature engineering manageable (i.e. less computationally expensive) we do the following:\n",
    "\n",
    "- for each of the 17 signals from the first faulted cavity (prediction from previous model) use `tsfresh` to compute a subset of available features\n",
    "    - specifically, compute the top contributing features using the `feature_importances_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass `cavityID` - from above - into the script below to load all 17 signals from _only the cavity that tripped_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_df = pd.DataFrame(columns=['Time', 'IMES', 'QMES', 'GMES', 'PMES', 'IASK', 'QASK', 'GASK',\n",
    "                                  'PASK', 'CRFP', 'CRFPP', 'CRRP', 'CRRPP', 'GLDE', 'PLDE', 'DETA2', 'CFQE2', 'DFQES', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some instances, the time stamp changes enough from one cavity to the next that you may need to use `time_format[:3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: D:\\RF WAVEFORMS\\rfw_tsf_extractor-Spring-2019\\waveform-data\\rf\\2L26\\2019_04_14\\100331.9\\R2Q2WFSharv.2019_04_14_100331.9.txt\n",
      "j = 0, k = 1, input shape = (8192, 19), fault shape = (8192, 19), time step = 0.05 ms\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "\n",
    "for i in filelist:\n",
    "    data_file_path =dir/i\n",
    "\n",
    "    log = pd.read_table(data_file_path, sep='\\t')\n",
    "\n",
    "    m, n = log.shape\n",
    "\n",
    "    for j in range(0, m):\n",
    "        k += 1\n",
    "        date, time = log.time[j].split(\" \", 1)\n",
    "\n",
    "        date_format = date.replace(\"/\", \"_\")\n",
    "        time_format = time.replace(\":\", \"\")\n",
    "\n",
    "        list1 = [time_format, '.', '?']\n",
    "\n",
    "        ct = os.path.join(module_path, log.zone[j], date_format, \"\".join(list1))\n",
    "\n",
    "        list2 = (cav_dict[log.zone[j]], cavityID_str, 'WFSharv.',\n",
    "                 date_format, '_', time_format[:3], '*', '.?.txt')\n",
    "\n",
    "        filename = \"\".join(list2)\n",
    "        #print(filename)\n",
    "\n",
    "        for f in glob.glob(os.path.join(ct, filename)):\n",
    "            df = pd.read_table(f, sep='\\t')\n",
    "            sLength = len(df['Time'])\n",
    "            tStep = (df.Time[2] - df.Time[1]) # in milliseconds\n",
    "            df['id'] = pd.Series(k, index=df.index)\n",
    "            df.columns = fault_df.columns\n",
    "            fault_df = fault_df.append(df)\n",
    "            print(\"Path: {0:s}\".format(f))\n",
    "            print(\"j = {0:}, k = {1:}, input shape = {2:}, fault shape = {3:}, time step = {4:3.2f} ms\".format(j, k, df.shape, fault_df.shape, tStep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input file has 8192 rows and 19 columns\n",
      "The input file has 1.0 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"The input file has\", fault_df.shape[0], \"rows and\", fault_df.shape[1], \"columns\")\n",
    "print(\"The input file has\", fault_df.shape[0]/8192, \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the computational load only the *top 50* features - based on previous analysis of a `RandomForestClassifier` - are computed and used as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['GMES__augmented_dickey_fuller__attr_\"usedlag\"',\n",
    " 'CRFP__maximum',\n",
    " 'CRFP__agg_linear_trend__f_agg_\"var\"__chunk_len_50__attr_\"stderr\"',\n",
    " 'GASK__fft_coefficient__coeff_64__attr_\"abs\"',\n",
    " 'CRFP__augmented_dickey_fuller__attr_\"teststat\"',\n",
    " 'GMES__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.4',\n",
    " 'CRFP__augmented_dickey_fuller__attr_\"pvalue\"',\n",
    " 'QMES__augmented_dickey_fuller__attr_\"usedlag\"',\n",
    " 'GMES__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.8__ql_0.6',\n",
    " 'PLDE__augmented_dickey_fuller__attr_\"usedlag\"',\n",
    " 'CRFP__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.8',\n",
    " 'CRFP__fft_coefficient__coeff_96__attr_\"abs\"',\n",
    " 'GMES__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.6',\n",
    " 'IMES__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.4',\n",
    " 'PLDE__partial_autocorrelation__lag_5',\n",
    " 'PLDE__minimum',\n",
    " 'GMES__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.6',\n",
    " 'QMES__approximate_entropy__m_2__r_0.5',\n",
    " 'PLDE__max_langevin_fixed_point__m_3__r_30',\n",
    " 'QASK__fft_coefficient__coeff_17__attr_\"abs\"',\n",
    " 'CRFP__agg_linear_trend__f_agg_\"var\"__chunk_len_50__attr_\"slope\"',\n",
    " 'CRFP__fft_coefficient__coeff_48__attr_\"abs\"',\n",
    " 'GMES__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.4',\n",
    " 'GMES__number_cwt_peaks__n_1',\n",
    " 'CRFP__ar_coefficient__k_10__coeff_0',\n",
    " 'QMES__approximate_entropy__m_2__r_0.7',\n",
    " 'GASK__fft_aggregated__aggtype_\"skew\"',\n",
    " 'CRRP__longest_strike_above_mean',\n",
    " 'PLDE__partial_autocorrelation__lag_6',\n",
    " 'IMES__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.6__ql_0.4',\n",
    " 'PLDE__fft_coefficient__coeff_82__attr_\"real\"',\n",
    " 'CRFP__ratio_beyond_r_sigma__r_1.5',\n",
    " 'PLDE__friedrich_coefficients__m_3__r_30__coeff_0',\n",
    " 'IMES__fft_coefficient__coeff_11__attr_\"abs\"',\n",
    " 'GASK__ratio_beyond_r_sigma__r_6',\n",
    " 'GASK__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.8',\n",
    " 'QASK__fft_aggregated__aggtype_\"skew\"',\n",
    " 'IMES__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.4',\n",
    " 'IASK__fft_aggregated__aggtype_\"skew\"',\n",
    " 'DETA2__longest_strike_above_mean',\n",
    " 'CRRPP__quantile__q_0.9',\n",
    " 'GASK__fft_aggregated__aggtype_\"centroid\"',\n",
    " 'QASK__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.0',\n",
    " 'PLDE__partial_autocorrelation__lag_2',\n",
    " 'DFQES__ratio_value_number_to_time_series_length',\n",
    " 'PLDE__fft_coefficient__coeff_43__attr_\"angle\"',\n",
    " 'CRFP__percentage_of_reoccurring_datapoints_to_all_datapoints',\n",
    " 'CRFP__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.8',\n",
    " 'QASK__spkt_welch_density__coeff_5',\n",
    " 'PLDE__autocorrelation__lag_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfresh\n",
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████████████████████████████████████████████| 17/17 [00:11<00:00,  1.47s/it]\n",
      "WARNING:tsfresh.utilities.dataframe_functions:The columns ['PLDE__friedrich_coefficients__m_3__r_30__coeff_0'\n",
      " 'PLDE__max_langevin_fixed_point__m_3__r_30'] did not have any finite values. Filling with zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "extraction_settings = settings.from_columns(top_features)\n",
    "\n",
    "# per https://github.com/blue-yonder/tsfresh/issues/478\n",
    "%time X_fault = extract_features(fault_df.astype(\"float64\"), column_id=\"id\", column_sort=\"Time\", \\\n",
    "                           impute_function=impute, kind_to_fc_parameters=extraction_settings, default_fc_parameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 1633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fault_master = X_fault[top_features]\n",
    "X_fault_master.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is required because the `RF_FAULT_top50` model was trained on standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fault_mean = np.load('RF_FAULT_top50_mean.npy')\n",
    "X_fault_var  = np.load('RF_FAULT_top50_var.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fault_master = (X_fault_master - X_fault_mean) / X_fault_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `RF_FAULT_top50` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_fault = joblib.load('RF_FAULT_top50_01292020.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `preprocessing` library to apply the `inverse_transform` to the numerical result and return a categorical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Controls Fault', 'E_Quench', 'Heat Riser Choke', 'Microphonics',\n",
       "       'Multi Cav turn off', 'Quench_100ms', 'Quench_3ms',\n",
       "       'Single Cav Turn off'], dtype=object)"
      ]
     },
     "execution_count": 1637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load('le_fault_classes.npy')\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1638,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trip was caused by a Single Cav Turn off fault\n",
      "Confidence level is: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "cavityFault = RF_fault.predict(X_fault_master)\n",
    "cavityFault_prob = RF_fault.predict_proba(X_fault_master)\n",
    "cavityFault_name = le.inverse_transform(cavityFault)\n",
    "cavityFault_name_str = cavityFault_name.astype(str)[0]\n",
    "\n",
    "print(\"The trip was caused by a\", cavityFault_name_str, \"fault\")\n",
    "fault_confidence = float(cavityFault_prob[0][cavityFault]*100)\n",
    "print(\"Confidence level is:\", fault_confidence,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trip event was initated by a Single Cav Turn off fault in cavity 2 of zone 2L26\n"
     ]
    }
   ],
   "source": [
    "print(\"The trip event was initated by a\", cavityFault_name_str ,\"fault in cavity\", int(cavityID), \"of zone\", log.zone[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cavity 2 ( 85.22 ) Single Cav Turn off ( 100.0 )\n"
     ]
    }
   ],
   "source": [
    "print('cavity', cavityID_str, '(',round(ID_confidence,2), ')', cavityFault_name_str, '(',round(fault_confidence,2),')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the notebook took: 0:00:13.109043 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "print(\"Executing the notebook took:\", datetime.now() - startTime, \"(h:mm:ss)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1646,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tsfresh\n",
    "#print(tsfresh.__version__)\n",
    "#from platform import python_version\n",
    "#print(python_version())\n",
    "#print(pd.__version__)\n",
    "#print(np.__version__)\n",
    "#import sklearn\n",
    "#sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1648,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
